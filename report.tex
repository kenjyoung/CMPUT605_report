\documentclass{article}
\usepackage{fullpage}
\usepackage{lastpage}
\usepackage{fancyhdr}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{enumerate}
\usepackage{xspace}
\usepackage{txfonts}
\begin{document}
\begin{center}
{\Large \textbf{CMPUT 605: Exploring Methods for Guiding Exploration in Learning to Play Hex}}
\end{center}
\section*{Introduction}
The aim of this project was initially to investigate a particular method for guiding exploration in the board-game Hex in order to facilitate learning of approximate win probabilities for each available move in a state using a deep convolutional neural network. This method was based on the idea that the goal of exploration should be to follow lines of play that maximally reduce the Bernoulli variance in the state win probability estimate (i.e. $P_w(1-P_w)$) . This initial goal was found to be seemingly infeasible (at least in the particular form which was investigated here). Therefore other methods of improving learning in the game of Hex were also investigated, including exploration guided by pseudo-counts and using information generated by an existing hex solver to guide the learning and exploration.

For brevity, throughout we will use the notation $x\leftsquigarrow y$ to denote a gradient descent update which modifies the parameters of $x$ in order to reduce $(x-y)^2$.

\section*{Exploration by Estimated Variance Reduction}
The first idea tried for guiding exploration was based on learning probability estimates for each move based on the assumption that each move being a win was an independent Bernoulli event with some initially unknown probability. Expanding from this the idea was to also learn an estimated reduction in state win probability variance that would result by taking each move, playing out the rest of the game to the end, and performing a full backup of all move probabilities discovered along the way.

let $Q(S,A)$ be our win probability (or action value) estimate for each action $A$ in a given state $S$. Ordinary ones-step Q-learning (for 2 player minimax games) performs updates of the form:
$$Q(S_t,A_t)\leftsquigarrow 1-\max_A Q(S_{t+1},A)$$
In order to iteratively improve the estimated action values. Notice that if the game were stochastic and $Q(S_t,A_t)$ was equal to the true win probability, this update would be at a fixed point hence in case of stochastic games it makes some sense to say that $Q(S_t,A_t)$ is our estimate of the true winning probability of each move. Hex on the other hand is deterministic so each move has a true win probability of either 1 or 0, hence it is perhaps less clear what intermediate values of $Q(S_t,A_t)$ imply, however it can be roughly thought of as our degree of belief about the state being a win.



\section*{Pseudocount Exploration}


\section*{Integrating a Hex solver with the Learning Process}


\end{document}